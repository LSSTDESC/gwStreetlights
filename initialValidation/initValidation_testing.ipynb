{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b341272e-2412-49e3-915c-6efcafafc261",
   "metadata": {},
   "source": [
    "# Initial validation\n",
    "\n",
    "## What is needed to run a gwCosmo Dark Siren analysis\n",
    "\n",
    "- `posterior_samples.json`: Should be a dict of the form `{id:path}` for the key being a galaxy ID and path being the path to posterior `.h5` file\n",
    "    - bilby results are in .json format, not h5\n",
    "    - Conversion function created below\n",
    "- `skymaps_dict.json`: Should be a dict of the form `{id:path}` for the key being a galaxy ID and path being the path to the corresponding skymap `.fits` file\n",
    "    - No skymaps are created from the current injections, but probably able to make a `.fits` skymap from the injection parameters fairly easily... I hope\n",
    "    - plot_skymap method in bilby.gw.result.CompactBinaryCoalescenceResult can do it fairly easily, so this can be addressed\n",
    "- Redshift LOS: An hdf5 file with the redshift line of sight prior\n",
    "    - This should be computed for LSST later on\n",
    "    - Q: can this be performed for DR2? Or validated for DR1?\n",
    "- Injections: an `h5` file of the CBC injections sampled\n",
    "    - Depending on the dataset of parameters that we want to test (GW footprint, photometric selection, etc.), this file will have to be remade every time\n",
    "    - It is probably easier to make a json file of the galaxy subset we care about, and then convert it using the conversion function below\n",
    "- parameter_dict: The parameters that we want to sample\n",
    "    - This will be a static dict, likely a json\n",
    "- A few other parameters, but the above are the important ones\n",
    "\n",
    "\n",
    "## What is needed to run a gwCosmo Bright Siren analysis\n",
    "- Most is just straightforward dictionary population + GW data from\n",
    "- It $\\textit{would}$ be interesting to have some mock data to try to run at scale, just to be ready\n",
    "\n",
    "## What is needed to do a joint standard siren analysis\n",
    "- Not a today problem, but this needs to be identified ASAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbeb4c6-02a2-40b5-9683-cd08ed827311",
   "metadata": {},
   "source": [
    "### To generate an `.h5` file from a `.json` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd4fe49d-c01a-40ea-b392-83b50dcab69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "def rewriteJson(path):\n",
    "    \"\"\"\n",
    "    A function to take a json file and convert it to an h5 file\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    path: Full path to json file\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    Full path to h5 file after being written\n",
    "    \"\"\"\n",
    "\n",
    "    # Load JSON file\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Isolate filename\n",
    "    fname = path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    # Isolate base path\n",
    "    basePath = \"/\"\n",
    "    for part in path.split(\"/\")[:-1]:\n",
    "        basePath = os.path.join(basePath,part)\n",
    "\n",
    "    h5Path = os.path.join(basePath,fname+\".h5\")\n",
    "    # Create HDF5 file\n",
    "    with h5py.File(h5Path, \"w\") as h5file:\n",
    "        # Previously, recursively_save_dict_contents was defined here\n",
    "    \n",
    "        recursively_save_dict_contents(h5file, data)\n",
    "    return h5Path\n",
    "\n",
    "def recursively_save_dict_contents(h5group, dict_data):\n",
    "    for key, value in dict_data.items():\n",
    "        if isinstance(value, dict):\n",
    "            subgroup = h5group.create_group(key)\n",
    "            recursively_save_dict_contents(subgroup, value)\n",
    "        else:\n",
    "            h5group.create_dataset(key, data=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "110916de-23d8-45bf-af34-bcae290a60c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/test/testing/testing2/testFile.h5'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afcd9103-5860-4083-946f-9d722361ea95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/test/testing/testing2'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cc758e0-e777-414f-92a8-d1d4f74c26a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"testFile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93319a-a4c6-4c78-8b9d-d5f38b5acf05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df0df5d-381d-40fb-bcd1-3ab68c61a3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
